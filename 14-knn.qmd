---
title: "Section 14: k-nearest-neighbors"
author: "Matt Higham"
format: 
  html:
    embed-resources: true
---

```{r}
set.seed(1119)
library(tidyverse)
library(here) 

pokemon <- read_csv(here("data", "pokemon_full.csv")) |>
  filter(Type %in% c("Steel", "Dark", "Fire", "Ice")) |>
  mutate(across(where(is.numeric), ~ (.x - min(.x)) /
                                 (max(.x) - min(.x)))) 
## mutate(across()) is scaling all of the quantiative variables
## according to the formula in the handwritten notes
## where(is.numeric) tells R to only scale numeric variables
```

```{r}
train_sample <- pokemon |>
  slice_sample(n = 75)
## randomly sample 75 rows of pokemon

test_sample <- anti_join(pokemon, train_sample)
## anti_join() says to filter pokemon to only contain
## rows not in the train_sample
```

```{r}
## install.packages("GGally")
library(GGally)
ggpairs(data = train_sample, columns = c("SpAtk", "height", "weight", "Type"))
## looking at side-by-side boxplots, it looks like
## all 3 predictors might be useful, but defense looks
## most useful as the boxplots are most different for defense

## the diagonal shows density plots which are liked smoothed histograms
```

```{r}
## install.packages("class")
library(class)

## create a data frame that only has the predictors
## that we will use
train_small <- train_sample |> select(HP, Attack, Defense, Speed, SpAtk, weight)
test_small <- test_sample |> select(HP, Attack, Defense, Speed, SpAtk, weight)

## put our response variable into a vector
train_cat <- train_sample$Type
test_cat <- test_sample$Type

knn_mod <- knn(train = train_small, test = test_small,
               cl = train_cat, k = 9)
knn_mod ## gives a vector of classifications for the test sample

## vector of classifications and the vector of "true" values
## rows are the predictions
## columns are the actual values
table(knn_mod, test_cat) 
```

Exercise 2. There were 11 pokemon that were correctly predicted to be fire that were actually fire pokemon.

Exercise 3. There were 3 pokemon that were incorrectly predicted to be dark that were actually fire pokemon.

Exercise 5.

```{r}
test_sample
(1 + 11 + 1 + 3) / 45

tab <- table(knn_mod, test_cat) 
sum(diag(tab)) / sum(tab)
## diag(tab) pulls only the diagonal elements from tab
```

Exercise 6. Change predictors and/or change k to see if you can improve the classification rate.

Exercise 7.

```{r}
## figure out the most common Type 
train_sample |> group_by(Type) |>
  summarise(n_type = n())

## figure out what proportion of Pokemon in test_sample
## are of the most common type in train_sample
test_sample |>
  mutate(is_fire = if_else(Type == "Fire",
                           true = 1,
                           false = 0)) |>
  relocate(is_fire) |>
  summarise(prop_fire = mean(is_fire))
## baseline: 0.356
```

Exercise 8. Smaller values of k have the advantage that you're using more similar cases in the training sample to classify an observation in the test sample. But, a value of k that is too small is subject to a lot of random chance in what neighbors happen to be around the test observation.

aside:
small k: low bias, high variance
large k: high bias, low variance

## Class Exercises

Class Exercise 1.

```{r}
library(tidyverse)

pokemon <- read_csv(here::here("data/pokemon_full.csv")) 
set.seed(1119)

## scale the quantitative predictors
pokemon_scaled <- pokemon |>
  mutate(across(where(is.numeric), ~ (.x - min(.x)) /
                  (max(.x) - min(.x))))

train_sample <- pokemon_scaled |>
  slice_sample(n = 550)
test_sample <- anti_join(pokemon_scaled, train_sample)

library(class)

train_pokemon <- train_sample |> select(HP, Attack, Defense, Speed,
                                        SpAtk, SpDef, height, weight)
test_pokemon <- test_sample |> select(HP, Attack, Defense, Speed,
                                      SpAtk, SpDef, height, weight)

## put our response variable into a vector
train_cat <- train_sample$Type
test_cat <- test_sample$Type

knn_mod <- knn(train = train_pokemon, test = test_pokemon,
               cl = train_cat, k = 19)
knn_mod

tab <- table(knn_mod, test_cat)
sum(diag(tab)) / sum(tab)
```

```{r}
get_class_rate <- function(k_val) {
  knn_mod <- knn(train = train_pokemon,
                 test = test_pokemon,
                 cl = train_cat, k = k_val)
  knn_mod
  
  tab <- table(knn_mod, test_cat)
  class_rate <- sum(diag(tab)) / sum(tab)
  
  return(class_rate)
}
get_class_rate(k_val = 19)

## b
k_vec <- 1:60
k_vec
## purrr::map()
class_rates <- map(k_vec, get_class_rate) |> unlist()

k_df <- tibble(k_vec, class_rates)
k_df
## make a line plot showing the classification rate
## for different values of k

train_sample |> group_by(Type) |>
  summarise(n_type = n()) |>
  arrange(desc(n_type))

## figure out what proportion of Pokemon in test_sample
## are of the most common type in train_sample
test_sample |>
  mutate(is_water = if_else(Type == "Water",
                           true = 1,
                           false = 0)) |>
  relocate(is_water) |>
  summarise(prop_water = mean(is_water))

ggplot(data = k_df, aes(x = k_vec,
                        y = class_rates)) +
  geom_line() +
  geom_hline(yintercept = 0.14, linetype = 2) +
  theme_minimal()
## add a horizontal line that gives the baseline
## classification rate for this example
```