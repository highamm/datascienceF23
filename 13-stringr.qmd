---
title: "Section 13: Tidy Text and `stringr`"
format: 
  html:
    embed-resources: true
---


## Section 13.1: `tidytext`

```{r}
library(tidyverse)
library(here)
beyonce <- read_csv(here("data/beyonce_lyrics.csv"))
head(beyonce)
beyonce

beyonce |> group_by(song_name) |> summarise(n_count = n())

library(tidytext)
## unnest_tokens breaks up a string (line) and extracts individual
## words from a particular string
beyonce_unnest <- beyonce |> unnest_tokens(output = "word", input = "line")
beyonce_unnest

beyonce_unnest <- beyonce_unnest |> mutate(word = str_to_lower(word))

beyonce_unnest |> group_by(word) |>
  summarise(n = n()) |>
  arrange(desc(n)) |>
  print(n = 25)

## issue: the most popular words are "stop" words: words that don't
## really have a meaning on their own.

stop_words

## remove the stop_words from beyonce_unnest
beyonce_stop <- anti_join(beyonce_unnest, stop_words, by = c("word" = "word"))

beyonce_sum <- beyonce_stop |> group_by(word) |>
  summarise(n = n()) |>
  arrange(desc(n)) |>
  print(n = 25)
beyonce_sum
```

Exercise 1

```{r}
other_stop_df <- tibble::tibble(other_stop = c("ooh", "gotta", "ya", "uh", "yeah", "hey"))

beyonce_new_stop <- anti_join(beyonce_stop, other_stop_df,
                              c("word" = "other_stop"))
```

Exercise 2

```{r}
beyonce_plot <- beyonce_new_stop |> group_by(word) |>
  summarise(n = n()) |>
  arrange(desc(n)) |>
  slice(1:20) |>
  mutate(word = fct_reorder(word, n))
ggplot(data = beyonce_plot, aes(x = word, y = n)) +
  geom_segment(aes(xend = word, y = 0, yend = n)) +
  geom_point() +
  coord_flip() +
  theme_minimal()
```

Exercise 3

```{r}
library(wordcloud)
beyonce_small <- beyonce_sum |> filter(n > 50)
wordcloud(beyonce_small$word, beyonce_small$n, 
          colors = brewer.pal(8, "Dark2"), scale = c(5, .2),
          random.order = FALSE, random.color = FALSE)
```

## Section 13.2: Introduction to `stringr`

```{r}
library(here)
library(tidyverse)
med_djok_df <- read_csv(here("data/med_djok.csv"))
head(med_djok_df)

str_detect(med_djok_df$point, pattern = "f")

str_detect(med_djok_df$point, pattern = "d@")

sum(str_detect(med_djok_df$point, pattern = "d@"))
```

```{r}
med_djok_df |> filter(str_detect(point, pattern = "@") == TRUE)

med_djok_df |> filter(str_detect(point, pattern = "@") == TRUE) |>
  mutate(error_type = case_when(str_detect(point, pattern = "d@") ~ "deep error",
                                   str_detect(point, pattern = "w@") ~ "wide error",
            str_detect(point, pattern = "n@") ~ "net error")) |>
  group_by(PtWinner, error_type) |>
  summarise(n_errors = n())
```

Exercise 4

```{r}
med_djok_df <- med_djok_df |>
  mutate(serve_location =
           case_when(str_detect(point, "^4") ~ "wide",
                     str_detect(point, "^5") ~ "body",
                     str_detect(point, "^6") ~ "down the center")) |>
  relocate(serve_location)
```

Your Turn 1, 2, 3

Your Turn 1

```{r}
med_djok_df |> group_by(Serving, serve_location) |>
  summarise(serve_count = n())
```

Your Turn 2

```{r}
med_djok_df |> group_by(Serving, serve_location) |>
  summarise(prop_win = mean(isSvrWinner))
```

Your Turn 3

```{r}
med_djok_df |> mutate(is_volley = str_detect(point,
                                             pattern = "[vzik]")) |>
  relocate(is_volley) |>
  summarise(prop_volley = mean(is_volley))
```